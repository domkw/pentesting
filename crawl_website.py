import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
import random
from fake_useragent import UserAgent

def crawl_website(url, output_file):
    visited_urls = set()
    internal_urls = set()
    user_agent = UserAgent()

    def crawl(url):
        visited_urls.add(url)
        headers = {'User-Agent': user_agent.random}
        try:
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and not href.startswith(('http', 'https')):
                    internal_url = urljoin(url, href)
                    if not internal_url.startswith(('tel:', 'mailto:')):
                        internal_urls.add(internal_url)
                        save_url_to_file(internal_url, output_file)  # حفظ الرابط في الملف
                        print(internal_url)  # عرض الرابط فور الحصول عليه
        except requests.exceptions.ConnectTimeout as e:
            print(f"Error connecting to {url}: {e}")

    def crawl_helper(url):
        if url not in visited_urls:
            crawl(url)

    crawl(url)

    while internal_urls:
        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(crawl_helper, list(internal_urls))
        internal_urls.clear()

def save_url_to_file(url, output_file):
    with open(output_file, 'a') as file:
        file.write(url + '\n')

def main():
    website = input("Enter website URL: ")
    output_file = input("Enter output file name: ")
    crawl_website(website, output_file)

if __name__ == "__main__":
    main()
