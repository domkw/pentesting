import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
import random
from pyfiglet import Figlet
from termcolor import colored
import tldextract

def crawl_website(url):
    visited_urls = set()
    internal_urls = set()
    user_agent_list = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    ]
    random.shuffle(user_agent_list)
    user_agent = user_agent_list[0]
    f = Figlet(font='slant')

    def crawl(url):
        visited_urls.add(url)
        headers = {'User-Agent': user_agent}
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'http://' + url  # إضافة بروتوكول إذا لم يكن موجودًا
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and not href.startswith(('http', 'https')):
                    internal_url = urljoin(url, href)
                    if not internal_url.startswith(('tel:', 'mailto:')):
                        internal_urls.add(internal_url)
                        save_url_to_file(internal_url)  # حفظ الرابط في الملف
                        print(colored(internal_url, 'green'))  # عرض الرابط فور الحصول عليه بلون أخضر
        except requests.exceptions.RequestException as e:
            print(colored(f"Error connecting to {url}: {e}", 'red'))  # عرض رسالة الخطأ بلون أحمر

    def crawl_helper(url):
        if url not in visited_urls:
            crawl(url)

    print(colored(f.renderText('sp bayan'), 'cyan'))  # عرض اللوغو بلون سماوي عند فتح البرنامج
    crawl(url)

    while internal_urls:
        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(crawl_helper, list(internal_urls))
        internal_urls.clear()

def save_url_to_file(url):
    domain = tldextract.extract(url).registered_domain
    output_file = f"{domain}.txt"
    with open(output_file, 'a') as file:
        file.write(url + '\n')

def main():
    website = input("Enter website URL: ")
    crawl_website(website)

if __name__ == "__main__":
    main()
