import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
import random
from pyfiglet import Figlet
from termcolor import colored
import tldextract
import argparse
import socket

def crawl_website(url):
    visited_urls = set()
    internal_urls = set()
    user_agent_list = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    ]

    f = Figlet(font='slant')

    def crawl(url):
        visited_urls.add(url)
        user_agent = random.choice(user_agent_list)
        headers = {'User-Agent': user_agent}
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'http://' + url  # إضافة بروتوكول HTTP إذا لم يكن موجودًا
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # التحقق من حالة الاستجابة
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and not href.startswith(('http', 'https')):
                    internal_url = urljoin(url, href)
                    if not internal_url.startswith(('tel:', 'mailto:')):
                        internal_urls.add(internal_url)
                        save_url_to_file(internal_url)  # حفظ الرابط في الملف
                        print(colored(internal_url, 'green'))  # عرض الرابط فور الحصول عليه بلون أخضر
        except requests.exceptions.RequestException as e:
            print(colored(f"Error connecting to {url}: {e}", 'red'))  # عرض رسالة الخطأ بلون أحمر

    def crawl_helper(url):
        if url not in visited_urls:
            crawl(url)

    print(colored(f.renderText('sp bayan'), 'cyan'))  # عرض اللوغو بلون سماوي عند فتح البرنامج
    crawl(url)

    while internal_urls:
        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(crawl_helper, list(internal_urls))
        internal_urls.clear()

def save_url_to_file(url):
    website_name = tldextract.extract(url).domain
    output_file = f"{website_name}.txt"
    with open(output_file, 'a') as file:
        file.write(url + '\n')

def main():
    parser = argparse.ArgumentParser(description='Crawl a website.')
    parser.add_argument('website', help='Website URL')
    args = parser.parse_args()
    crawl_website(args.website)

if __name__ == "__main__":
    main()
