import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
import random
from fake_useragent import UserAgent

def crawl_website(url):
    visited_urls = set()
    internal_urls = set()
    user_agent = UserAgent()

    def crawl(url):
        visited_urls.add(url)
        headers = {'User-Agent': user_agent.random}
        try:
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and not href.startswith(('http', 'https')):
                    internal_url = urljoin(url, href)
                    if not internal_url.startswith(('tel:', 'mailto:')):
                        internal_urls.add(internal_url)
                        print(internal_url)  # طباعة الرابط فور الحصول عليه
        except requests.exceptions.ConnectTimeout as e:
            print(f"Error connecting to {url}: {e}")

    def crawl_helper(url):
        if url not in visited_urls:
            crawl(url)

    crawl(url)

    while internal_urls:
        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(crawl_helper, list(internal_urls))
        internal_urls.clear()

url = "https://example.com"
crawl_website(url)
