import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor

def crawl_website(url):
    visited_urls = set()
    internal_urls = set()

    def crawl(url):
        visited_urls.add(url)
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        for link in soup.find_all('a'):
            href = link.get('href')
            if href and not href.startswith(('http', 'https')):
                internal_url = urljoin(url, href)
                if not internal_url.startswith(('tel:', 'mailto:')):
                    internal_urls.add(internal_url)

    def crawl_helper(url):
        if url not in visited_urls:
            crawl(url)

    crawl(url)

    while internal_urls:
        with ThreadPoolExecutor(max_workers=4) as executor:  # تعديل عدد العمليات الفرعية حسب الحاجة
            executor.map(crawl_helper, list(internal_urls))
        internal_urls.clear()

    return visited_urls

url = "https://jude.edu.sy"
visited_urls = crawl_website(url)

print("Discovered URLs:")
for url in visited_urls:
    print(url)
